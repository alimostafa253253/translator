{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UPVy_Rd60MqS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "fTOYxWVV0FDg",
    "outputId": "c9c3f0eb-2134-4eab-c797-d5a075363f37"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Arabic</th>\n",
       "      <th>English</th>\n",
       "      <th>French</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>Hello</td>\n",
       "      <td>Salut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>صباح الخير</td>\n",
       "      <td>Good Morning</td>\n",
       "      <td>Bonjour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>شكرا</td>\n",
       "      <td>Thank You</td>\n",
       "      <td>Merci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>أنا آسف</td>\n",
       "      <td>Sorry</td>\n",
       "      <td>Désolé</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>أنا احبك</td>\n",
       "      <td>I Love You</td>\n",
       "      <td>Je t'aime</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Arabic       English     French\n",
       "0       مرحبا         Hello      Salut\n",
       "1  صباح الخير  Good Morning    Bonjour\n",
       "2        شكرا     Thank You      Merci\n",
       "3     أنا آسف         Sorry     Désolé\n",
       "4    أنا احبك    I Love You  Je t'aime"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    'Arabic': ['مرحبا', 'صباح الخير', 'شكرا', 'أنا آسف', 'أنا احبك'],\n",
    "    'English': ['Hello', 'Good Morning', 'Thank You', 'Sorry', 'I Love You'],\n",
    "    'French': ['Salut', 'Bonjour', 'Merci', 'Désolé', 'Je t\\'aime']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ig8dRuea0-Ta",
    "outputId": "862f1661-8985-444e-aa87-c1461463a6aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\win\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aHXjji5N7eX6"
   },
   "outputs": [],
   "source": [
    "# Define stop words for each language\n",
    "stop_words_arabic = set(stopwords.words('arabic'))\n",
    "stop_words_english = set(stopwords.words('english'))\n",
    "stop_words_french = set(stopwords.words('french'))\n",
    "import string\n",
    "string.punctuation\n",
    "import string\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text: str):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Apply the function to remove punctuation from each column\n",
    "df['Arabic'] = df['Arabic'].apply(remove_punctuation)\n",
    "df['English'] = df['English'].apply(remove_punctuation)\n",
    "df['French'] = df['French'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OyU06TeK7MC2"
   },
   "outputs": [],
   "source": [
    "# Preprocessing: Lowercase and remove stop words\n",
    "def remove_stopwords(text, lang):\n",
    "    if lang == 'arabic':\n",
    "        return ' '.join([word for word in text.split() if word not in stop_words_arabic])\n",
    "    elif lang == 'english':\n",
    "        return ' '.join([word for word in text.split() if word.lower() not in stop_words_english])\n",
    "    elif lang == 'french':\n",
    "        return ' '.join([word for word in text.split() if word.lower() not in stop_words_french])\n",
    "    return text\n",
    "\n",
    "df['Arabic'] = df['Arabic'].str.lower().apply(lambda x: remove_stopwords(x, 'arabic'))\n",
    "df['English'] = df['English'].str.lower().apply(lambda x: remove_stopwords(x, 'english'))\n",
    "df['French'] = df['French'].str.lower().apply(lambda x: remove_stopwords(x, 'french'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "gBva-WbmBfji"
   },
   "outputs": [],
   "source": [
    "# Add start and end tokens to target languages\n",
    "df['English'] = df['English'].apply(lambda x: '<start> ' + x + ' <end>')\n",
    "df['French'] = df['French'].apply(lambda x: '<start> ' + x + ' <end>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           <start> hello <end>\n",
       "1    <start> good morning <end>\n",
       "2           <start> thank <end>\n",
       "3           <start> sorry <end>\n",
       "4            <start> love <end>\n",
       "Name: English, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TzBhzTOI7wv3"
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer_src = Tokenizer()\n",
    "tokenizer_src.fit_on_texts(df['Arabic'])\n",
    "source_sequences = tokenizer_src.texts_to_sequences(df['Arabic'])\n",
    "\n",
    "tokenizer_tgt_en = Tokenizer()\n",
    "tokenizer_tgt_en.fit_on_texts(df['English'])\n",
    "target_sequences_en = tokenizer_tgt_en.texts_to_sequences(df['English'])\n",
    "\n",
    "tokenizer_tgt_fr = Tokenizer()\n",
    "tokenizer_tgt_fr.fit_on_texts(df['French'])\n",
    "target_sequences_fr = tokenizer_tgt_fr.texts_to_sequences(df['French'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NmOVcmYw764b"
   },
   "outputs": [],
   "source": [
    "# Padding sequences\n",
    "max_length = max(max(len(seq) for seq in source_sequences),\n",
    "                 max(len(seq) for seq in target_sequences_en),\n",
    "                 max(len(seq) for seq in target_sequences_fr))\n",
    "\n",
    "source_sequences = pad_sequences(source_sequences, maxlen=max_length, padding='pre')\n",
    "target_sequences_en = pad_sequences(target_sequences_en, maxlen=max_length, padding='pre')\n",
    "target_sequences_fr = pad_sequences(target_sequences_fr, maxlen=max_length, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7bnXBLJ08M-l"
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train_src_to_en, X_test_src_to_en, y_train_src_to_en, y_test_src_to_en = train_test_split(source_sequences, target_sequences_en, test_size=0.2, random_state=42)\n",
    "X_train_en_to_src,X_test_en_to_src,y_train_en_to_src,y_test_en_to_src=train_test_split(target_sequences_en,source_sequences,test_size=0.2,random_state=42)\n",
    "X_train_src_to_fr, X_test_src_to_fr, y_train_src_to_fr, y_test_src_to_fr = train_test_split(source_sequences, target_sequences_fr, test_size=0.2, random_state=42)\n",
    "X_train_fr_to_src,X_test_fr_to_src,y_train_fr_to_src,y_test_fr_to_src=train_test_split(target_sequences_fr,source_sequences,test_size=0.2,random_state=42)\n",
    "X_train_fr_to_en,X_test_fr_to_en,y_train_fr_to_en,y_test_fr_to_en =train_test_split(target_sequences_fr,target_sequences_en,test_size=0.2,random_state=42)\n",
    "X_train_en_to_fr,X_test_en_to_fr,y_train_en_to_fr,y_test_en_to_fr=train_test_split(target_sequences_en,target_sequences_fr,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CO6Uh8VH8Tju"
   },
   "outputs": [],
   "source": [
    "# Define Encoder\n",
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(enc_units, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        enc_output, state_h, state_c = self.lstm(x)\n",
    "        return enc_output, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nelHDIRF8cCP"
   },
   "outputs": [],
   "source": [
    "# Define Decoder\n",
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, x, enc_output, state_h, state_c):\n",
    "        x = self.embedding(x)\n",
    "        dec_output, dec_state_h, dec_state_c = self.lstm(x, initial_state=[state_h, state_c])\n",
    "        output = self.fc(dec_output)\n",
    "        return output, dec_state_h, dec_state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zzHFe60Y8kwr"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "vocab_size_src    = len(tokenizer_src.word_index) + 1\n",
    "vocab_size_tgt_en = len(tokenizer_tgt_en.word_index) + 1\n",
    "vocab_size_tgt_fr = len(tokenizer_tgt_fr.word_index) + 1\n",
    "embedding_dim = 256\n",
    "units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Kwh7nW-99cs6"
   },
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "encoder_src = Encoder(vocab_size_src, embedding_dim, units)\n",
    "encoder_en = Encoder(vocab_size_tgt_en,embedding_dim,units)\n",
    "encoder_fr = Encoder(vocab_size_tgt_fr,embedding_dim,units)\n",
    "decoder_src=Decoder(vocab_size_src,embedding_dim,units)\n",
    "decoder_en = Decoder(vocab_size_tgt_en, embedding_dim, units)\n",
    "decoder_fr = Decoder(vocab_size_tgt_fr, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UDcYBRXo9dph"
   },
   "outputs": [],
   "source": [
    "# Compile the models\n",
    "encoder_src.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "encoder_en.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "encoder_fr.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "decoder_src.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "decoder_en.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "decoder_fr.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eCsXLhGe9ePO"
   },
   "outputs": [],
   "source": [
    "# Prepare the target data for training \n",
    "# Shift the target sequences for teacher forcing\n",
    "y_train_src_to_en_input = y_train_src_to_en[:, :-1]  # All but the last word\n",
    "y_train_src_to_en_output = y_train_src_to_en[:, 1:]    # All but the first word\n",
    "y_train_en_to_src_input=y_train_en_to_src[:,:-1]  # All but the last word\n",
    "y_train_en_to_src_output=y_train_en_to_src[:,1:]  # All but the first word\n",
    "y_train_src_to_fr_input = y_train_src_to_fr[:, :-1]  # All but the last word\n",
    "y_train_src_to_fr_output = y_train_src_to_fr[:, 1:]    # All but the first word\n",
    "y_train_fr_to_src_input=y_train_fr_to_src[:,:-1]  # All but the last word\n",
    "y_train_fr_to_src_output=y_train_fr_to_src[:,1:]  # All but the first word\n",
    "y_train_en_to_fr_input= y_train_en_to_fr[:,:-1]  # All but the last word\n",
    "y_train_en_to_fr_output=y_train_en_to_fr[:,1:]  # All but the first word\n",
    "y_train_fr_to_en_input=y_train_fr_to_en[:,:-1]  # All but the last word\n",
    "y_train_fr_to_en_output=y_train_fr_to_en[:,1:]   # All but the first word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 8],\n",
       "       [0, 1, 6],\n",
       "       [0, 1, 3],\n",
       "       [0, 1, 7]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_src_to_en_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 8, 2],\n",
       "       [0, 1, 6, 2],\n",
       "       [0, 1, 3, 2],\n",
       "       [0, 1, 7, 2]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_src_to_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "3wVFwEZW62b7"
   },
   "outputs": [],
   "source": [
    "# Define a function to train the model\n",
    "def train_model(encoder, decoder, X, y_input, y_output, epochs=100):\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            # Get the input and output for the current sample\n",
    "            encoder_input = np.array([X[i]])\n",
    "            decoder_input = np.array([y_input[i]])\n",
    "\n",
    "            # Initialize the hidden state of the encoder\n",
    "            enc_output, state_h, state_c = encoder(encoder_input)\n",
    "\n",
    "            # Initialize the decoder input and states\n",
    "            with tf.GradientTape() as tape:\n",
    "                decoder_output, _, _ = decoder(decoder_input, enc_output, state_h, state_c)\n",
    "\n",
    "                # Calculate the loss (using sparse categorical crossentropy)\n",
    "                # Reshape decoder_output to match y_output[i]'s shape\n",
    "                # The change is in the next line: using decoder_output[0]\n",
    "                loss = tf.keras.losses.sparse_categorical_crossentropy(y_output[i], decoder_output[0])\n",
    "\n",
    "            # Backpropagation\n",
    "            gradients = tape.gradient(loss, decoder.trainable_variables)\n",
    "            decoder.optimizer.apply_gradients(zip(gradients, decoder.trainable_variables))\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {np.mean(loss.numpy())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-fXZVIT93_r",
    "outputId": "d9f818b8-644b-4aea-b965-175c532d5144",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Arabic to English Decoder...\n",
      "Epoch 1/20, Loss: 2.135546922683716\n",
      "Epoch 2/20, Loss: 1.9705390930175781\n",
      "Epoch 3/20, Loss: 1.6780797243118286\n",
      "Epoch 4/20, Loss: 1.4397263526916504\n",
      "Epoch 5/20, Loss: 1.3516579866409302\n",
      "Epoch 6/20, Loss: 1.1341012716293335\n",
      "Epoch 7/20, Loss: 0.9341280460357666\n",
      "Epoch 8/20, Loss: 0.7627288699150085\n",
      "Epoch 9/20, Loss: 0.6189457178115845\n",
      "Epoch 10/20, Loss: 0.5438634157180786\n",
      "Epoch 11/20, Loss: 0.49920961260795593\n",
      "Epoch 12/20, Loss: 0.47243019938468933\n",
      "Epoch 13/20, Loss: 0.4616376459598541\n",
      "Epoch 14/20, Loss: 0.46247777342796326\n",
      "Epoch 15/20, Loss: 0.46829068660736084\n",
      "Epoch 16/20, Loss: 0.47297847270965576\n",
      "Epoch 17/20, Loss: 0.4733855724334717\n",
      "Epoch 18/20, Loss: 0.4699072539806366\n",
      "Epoch 19/20, Loss: 0.46495839953422546\n",
      "Epoch 20/20, Loss: 0.46083369851112366\n"
     ]
    }
   ],
   "source": [
    "# Train the English decoder\n",
    "print(\"Training Arabic to English Decoder...\")\n",
    "train_model(encoder_src, decoder_en, X_train_src_to_en, y_train_src_to_en_input, y_train_src_to_en_output, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training  English to Arabic Decoder...\n",
      "Epoch 1/20, Loss: 1.7277189493179321\n",
      "Epoch 2/20, Loss: 1.6028165817260742\n",
      "Epoch 3/20, Loss: 1.4762330055236816\n",
      "Epoch 4/20, Loss: 1.3389692306518555\n",
      "Epoch 5/20, Loss: 1.1505546569824219\n",
      "Epoch 6/20, Loss: 0.9338092803955078\n",
      "Epoch 7/20, Loss: 0.7266533970832825\n",
      "Epoch 8/20, Loss: 0.5697762966156006\n",
      "Epoch 9/20, Loss: 0.5232869982719421\n",
      "Epoch 10/20, Loss: 0.5225786566734314\n",
      "Epoch 11/20, Loss: 0.5133489370346069\n",
      "Epoch 12/20, Loss: 0.4995051622390747\n",
      "Epoch 13/20, Loss: 0.4935086667537689\n",
      "Epoch 14/20, Loss: 0.4936009645462036\n",
      "Epoch 15/20, Loss: 0.4928872585296631\n",
      "Epoch 16/20, Loss: 0.4894077777862549\n",
      "Epoch 17/20, Loss: 0.4853297770023346\n",
      "Epoch 18/20, Loss: 0.4826402962207794\n",
      "Epoch 19/20, Loss: 0.4814740717411041\n",
      "Epoch 20/20, Loss: 0.4808904230594635\n"
     ]
    }
   ],
   "source": [
    "print(\"Training  English to Arabic Decoder...\")\n",
    "train_model(encoder_en, decoder_src, X_train_en_to_src, y_train_en_to_src_input, y_train_en_to_src_output, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AM8mIcRE-BS_",
    "outputId": "14c84050-315f-4f0c-9b48-118c875e8328",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Arabic to French Decoder...\n",
      "Epoch 1/20, Loss: 2.0185165405273438\n",
      "Epoch 2/20, Loss: 1.8590484857559204\n",
      "Epoch 3/20, Loss: 1.5889023542404175\n",
      "Epoch 4/20, Loss: 1.388357162475586\n",
      "Epoch 5/20, Loss: 1.3023297786712646\n",
      "Epoch 6/20, Loss: 1.0974689722061157\n",
      "Epoch 7/20, Loss: 0.9193122386932373\n",
      "Epoch 8/20, Loss: 0.7572290301322937\n",
      "Epoch 9/20, Loss: 0.6245836615562439\n",
      "Epoch 10/20, Loss: 0.5523881912231445\n",
      "Epoch 11/20, Loss: 0.5054731369018555\n",
      "Epoch 12/20, Loss: 0.47573187947273254\n",
      "Epoch 13/20, Loss: 0.46284064650535583\n",
      "Epoch 14/20, Loss: 0.46258974075317383\n",
      "Epoch 15/20, Loss: 0.46810850501060486\n",
      "Epoch 16/20, Loss: 0.4731982946395874\n",
      "Epoch 17/20, Loss: 0.4743584394454956\n",
      "Epoch 18/20, Loss: 0.47142815589904785\n",
      "Epoch 19/20, Loss: 0.46653202176094055\n",
      "Epoch 20/20, Loss: 0.4621165990829468\n"
     ]
    }
   ],
   "source": [
    "# Train the French decoder\n",
    "print(\"Training Arabic to French Decoder...\")\n",
    "train_model(encoder_src, decoder_fr, X_train_src_to_fr, y_train_src_to_fr_input, y_train_src_to_fr_output, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Training French to Arabic Decoder\n",
      "Epoch 1/20, Loss: 0.48563122749328613\n",
      "Epoch 2/20, Loss: 0.48548802733421326\n",
      "Epoch 3/20, Loss: 0.48573175072669983\n",
      "Epoch 4/20, Loss: 0.48564672470092773\n",
      "Epoch 5/20, Loss: 0.48502257466316223\n",
      "Epoch 6/20, Loss: 0.4840656518936157\n",
      "Epoch 7/20, Loss: 0.4831100404262543\n",
      "Epoch 8/20, Loss: 0.4823780953884125\n",
      "Epoch 9/20, Loss: 0.481898695230484\n",
      "Epoch 10/20, Loss: 0.4815733730792999\n",
      "Epoch 11/20, Loss: 0.4812770187854767\n",
      "Epoch 12/20, Loss: 0.4809330701828003\n",
      "Epoch 13/20, Loss: 0.48053303360939026\n",
      "Epoch 14/20, Loss: 0.4801109731197357\n",
      "Epoch 15/20, Loss: 0.4797076880931854\n",
      "Epoch 16/20, Loss: 0.4793466627597809\n",
      "Epoch 17/20, Loss: 0.4790273606777191\n",
      "Epoch 18/20, Loss: 0.47873637080192566\n",
      "Epoch 19/20, Loss: 0.478456974029541\n",
      "Epoch 20/20, Loss: 0.47818002104759216\n"
     ]
    }
   ],
   "source": [
    "print(\" Training French to Arabic Decoder\")\n",
    "train_model(encoder_fr,decoder_src,X_train_fr_to_src,y_train_fr_to_src_input,y_train_fr_to_src_output,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training French to English Decoder\n",
      "Epoch 1/20, Loss: 0.47043928503990173\n",
      "Epoch 2/20, Loss: 0.47077512741088867\n",
      "Epoch 3/20, Loss: 0.47341135144233704\n",
      "Epoch 4/20, Loss: 0.47628292441368103\n",
      "Epoch 5/20, Loss: 0.47798100113868713\n",
      "Epoch 6/20, Loss: 0.4780600070953369\n",
      "Epoch 7/20, Loss: 0.4768620431423187\n",
      "Epoch 8/20, Loss: 0.47508731484413147\n",
      "Epoch 9/20, Loss: 0.47339990735054016\n",
      "Epoch 10/20, Loss: 0.47219112515449524\n",
      "Epoch 11/20, Loss: 0.4715368449687958\n",
      "Epoch 12/20, Loss: 0.47128668427467346\n",
      "Epoch 13/20, Loss: 0.4712033271789551\n",
      "Epoch 14/20, Loss: 0.4710737466812134\n",
      "Epoch 15/20, Loss: 0.4707850217819214\n",
      "Epoch 16/20, Loss: 0.4703216552734375\n",
      "Epoch 17/20, Loss: 0.4697379767894745\n",
      "Epoch 18/20, Loss: 0.46911129355430603\n",
      "Epoch 19/20, Loss: 0.46850743889808655\n",
      "Epoch 20/20, Loss: 0.4679598808288574\n"
     ]
    }
   ],
   "source": [
    "print(\"Training French to English Decoder\")\n",
    "train_model(encoder_fr,decoder_en,X_train_fr_to_en,y_train_fr_to_en_input,y_train_fr_to_en_output,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training English to French Decoder\n",
      "Epoch 1/20, Loss: 0.47094595432281494\n",
      "Epoch 2/20, Loss: 0.4714568853378296\n",
      "Epoch 3/20, Loss: 0.47424137592315674\n",
      "Epoch 4/20, Loss: 0.4772104024887085\n",
      "Epoch 5/20, Loss: 0.47894248366355896\n",
      "Epoch 6/20, Loss: 0.47899118065834045\n",
      "Epoch 7/20, Loss: 0.4777046740055084\n",
      "Epoch 8/20, Loss: 0.475801944732666\n",
      "Epoch 9/20, Loss: 0.47397705912590027\n",
      "Epoch 10/20, Loss: 0.47264906764030457\n",
      "Epoch 11/20, Loss: 0.47191622853279114\n",
      "Epoch 12/20, Loss: 0.47163423895835876\n",
      "Epoch 13/20, Loss: 0.471556693315506\n",
      "Epoch 14/20, Loss: 0.47145554423332214\n",
      "Epoch 15/20, Loss: 0.4711962044239044\n",
      "Epoch 16/20, Loss: 0.4707476794719696\n",
      "Epoch 17/20, Loss: 0.4701574146747589\n",
      "Epoch 18/20, Loss: 0.4695064127445221\n",
      "Epoch 19/20, Loss: 0.4688683748245239\n",
      "Epoch 20/20, Loss: 0.46829017996788025\n"
     ]
    }
   ],
   "source": [
    "print(\"Training English to French Decoder\")\n",
    "train_model(encoder_en,decoder_fr,X_train_en_to_fr,y_train_en_to_fr_input,y_train_en_to_fr_output,epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "M47D_TPG_ROJ"
   },
   "outputs": [],
   "source": [
    "# Evaluate the models on the test set (optional)\n",
    "def evaluate_model(encoder, decoder, X, y_input, y_output):\n",
    "    total_loss = 0\n",
    "    for i in range(len(X)):\n",
    "        encoder_input = np.array([X[i]])\n",
    "        decoder_input = np.array([y_input[i]])\n",
    "\n",
    "        enc_output, state_h, state_c = encoder(encoder_input)\n",
    "\n",
    "        # Calculate the loss\n",
    "        decoder_output, _, _ = decoder(decoder_input, enc_output, state_h, state_c)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_output[i], decoder_output[0])\n",
    "        total_loss += np.mean(loss.numpy())\n",
    "\n",
    "    print(f'Test Loss: {total_loss / len(X)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wt0Bw0uaEiY9",
    "outputId": "1d399922-d7af-43e1-86e2-f9b05f7ddc18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Arabic to English Decoder...\n",
      "Test Loss: 2.257819175720215\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the English decoder\n",
    "print(\"Evaluating Arabic to English Decoder...\")\n",
    "evaluate_model(encoder_src, decoder_en, X_test_src_to_en, y_test_src_to_en[:, :-1], y_test_src_to_en[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating English to Arabic Decoder\n",
      "Test Loss: 2.9857609272003174\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating English to Arabic Decoder\")\n",
    "evaluate_model(encoder_en,decoder_src,X_test_en_to_src,y_test_en_to_src[:,:-1],y_test_en_to_src[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HtkZGTZrElsw",
    "outputId": "a24c9a91-15f0-4a3d-a83f-af7663acf494"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Arabic to French Decoder...\n",
      "Test Loss: 3.037219285964966\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the French decoder\n",
    "print(\"Evaluating Arabic to French Decoder...\")\n",
    "evaluate_model(encoder_src, decoder_fr, X_test_src_to_fr, y_test_src_to_fr[:, :-1], y_test_src_to_fr[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating French to Arabic Decoder\n",
      "Test Loss: 2.9436838626861572\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating French to Arabic Decoder\")\n",
    "evaluate_model(encoder_fr,decoder_src,X_test_fr_to_src,y_test_fr_to_src[:,:-1],y_test_fr_to_src[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating French to English Decoder\n",
      "Test Loss: 1.972615122795105\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating French to English Decoder\")\n",
    "evaluate_model(encoder_fr,decoder_en,X_test_fr_to_en,y_test_fr_to_en[:,:-1],y_test_fr_to_en[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating English to French Decoder\n",
      "Test Loss: 2.8439056873321533\n"
     ]
    }
   ],
   "source": [
    "print(\"Evaluating English to French Decoder\")\n",
    "evaluate_model(encoder_en,decoder_fr,X_test_en_to_fr,y_test_en_to_fr[:,:-1],y_test_en_to_fr[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "8biO3KM-E6eS"
   },
   "outputs": [],
   "source": [
    "# Define the inference function\n",
    "def translate_sentence(sentence, encoder, decoder, tokenizer_tgt):\n",
    "    # Preprocess the input sentence\n",
    "    sequence = tokenizer_src.texts_to_sequences([sentence])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
    "\n",
    "    # Get the encoder output\n",
    "    enc_output, state_h, state_c = encoder(padded_sequence)\n",
    "\n",
    "    # Start decoding\n",
    "    # Check if '<start>' is in word_index, if not, use 1 (often the default for unknown)\n",
    "    start_token_index = tokenizer_tgt.word_index.get('<start>', 1)\n",
    "    target_seq = np.array([[start_token_index]])  # Start token\n",
    "    translated_sentence = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        output, state_h, state_c = decoder(target_seq, enc_output, state_h, state_c)\n",
    "        predicted_word_index = np.argmax(output[0, -1, :])\n",
    "        predicted_word = tokenizer_tgt.index_word.get(predicted_word_index, '')\n",
    "\n",
    "        if predicted_word == '<end>':  # End token\n",
    "            break\n",
    "\n",
    "        translated_sentence.append(predicted_word)\n",
    "        target_seq = np.array([[predicted_word_index]])\n",
    "\n",
    "    return ' '.join(translated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQ6iusSeEuOK",
    "outputId": "1d9a1b3c-f485-4df4-8f20-6e2733e5f93e"
   },
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# arabic_sentence = 'مرحبا'\n",
    "# print(\"Translating Arabic to English:\")\n",
    "# translated_en = translate_sentence(arabic_sentence, encoder, decoder_en, tokenizer_tgt_en)\n",
    "# print(f\"Translation (English): {translated_en}\")\n",
    "\n",
    "# print(\"Translating Arabic to French:\")\n",
    "# translated_fr = translate_sentence(arabic_sentence, encoder, decoder_fr, tokenizer_tgt_fr)\n",
    "# print(f\"Translation (French): {translated_fr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "-K2F05yUFAzR"
   },
   "outputs": [],
   "source": [
    "# Save the models\n",
    "# encoder.save('encoder_model.h5')\n",
    "# decoder_en.save('decoder_en_model.h5')\n",
    "# decoder_fr.save('decoder_fr_model.h5')\n",
    "\n",
    "# Load the models (if needed)\n",
    "# encoder = tf.keras.models.load_model('encoder_model.h5', custom_objects={'Encoder': Encoder})\n",
    "# decoder_en = tf.keras.models.load_model('decoder_en_model.h5', custom_objects={'Decoder': Decoder})\n",
    "# decoder_fr = tf.keras.models.load_model('decoder_fr_model.h5', custom_objects={'Decoder': Decoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "V8JfSvZXFutz"
   },
   "outputs": [],
   "source": [
    "def translate_user_input():\n",
    "    user_sentence = input(\"Enter a sentence in Arabic, English, or French: \").strip().lower()\n",
    "    target_language = input(\"Translate to (English/French/Arabic): \").strip().lower()\n",
    "\n",
    "    # Determine source and target tokenizers\n",
    "    if target_language == 'english':\n",
    "        tokenizer_tgt = tokenizer_tgt_en\n",
    "        decoder = decoder_en\n",
    "    elif target_language == 'french':\n",
    "        tokenizer_tgt = tokenizer_tgt_fr\n",
    "        decoder = decoder_fr\n",
    "    elif target_language == 'arabic':\n",
    "        tokenizer_tgt = tokenizer_src\n",
    "        decoder = decoder_src\n",
    "    else:\n",
    "        print(\"Invalid target language. Choose from English, French, or Arabic.\")\n",
    "        return\n",
    "\n",
    "    # Detect source language and set encoder\n",
    "    if all(word in tokenizer_src.word_index for word in user_sentence.split()):\n",
    "        print(\"Detected source language: Arabic\")\n",
    "        encoder = encoder_src\n",
    "    elif all(word in tokenizer_tgt_en.word_index for word in user_sentence.split()):\n",
    "        print(\"Detected source language: English\")\n",
    "        encoder = encoder_en\n",
    "    elif all(word in tokenizer_tgt_fr.word_index for word in user_sentence.split()):\n",
    "        print(\"Detected source language: French\")\n",
    "        encoder = encoder_fr\n",
    "    else:\n",
    "        print(\"Unsupported or mixed language. Please try again.\")\n",
    "        return\n",
    "\n",
    "    # Translate sentence\n",
    "    translated_sentence = translate_sentence(user_sentence, encoder, decoder, tokenizer_tgt)\n",
    "    print(f\"Translated Sentence ({target_language.capitalize()}): {translated_sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "pGdFnAzQG9s8"
   },
   "outputs": [],
   "source": [
    "# Define Reverse Decoder for translating from English/French to Arabic\n",
    "class ReverseDecoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(ReverseDecoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = Dense(vocab_size, activation='softmax')\n",
    "\n",
    "    def call(self, x, enc_output, state_h, state_c):\n",
    "        x = self.embedding(x)\n",
    "        dec_output, dec_state_h, dec_state_c = self.lstm(x, initial_state=[state_h, state_c])\n",
    "        output = self.fc(dec_output)\n",
    "        return output, dec_state_h, dec_state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vQah4pBQHAhP",
    "outputId": "1ee2dc2e-bdf4-40a2-d6bd-aa91677d91e4"
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "translate_user_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A7F8a9MWMSYn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
